---
layout:     post
title:      "回归分析初步"
subtitle:   
date:       2018-01-15
author:     "Govind"
header-img: "img/post_bg/19.jpg"
tags:
    - 回归分析
---



## 回归分析初步

回归分析[^1]通过建立模型来研究变量之间相互关系的密切程度、结构状态及进行模型预测的一种有效工具。

![回归分析](/img/md_imgs/回归分析.png)

**关系** ：函数关系、相关关系。

`函数关系`是`确定性关系`，例如：Y=3+10\*X；而`相关关系`是`非确定性关系` 。`回归分析中研究的是相关关系`，即非确定性关系。虽然可以用一条直线描述出x与y之间的关系，但是由于随机因素的影响使得散点不会严格的落在直线上，即知道自变量的值，是不能准备的预测因变量的值，可能只能预测出因变量的大致范围，因为随机因素的影响，实际值与预测值会有出入。`自变量与因变量的相关性可以通过相关系数描述`。

**相关系数** 

通常用相关系数`衡量线性相关性`的强弱。注意用线性相关系数描述非线性相关性的话，结果是不准确的！相关系数的计算公式如下(XY的协方差比上X的标准差与Y的标准差的乘积)：

![回归分析1](/img/md_imgs/回归分析1.png)
$$
r_{xy} = \frac{\sum(X_i-\overline X)(Y_i-\overline Y)}{\sqrt{\sum(X_i-\overline X)^2\sum(Y_i-\overline Y)^2}}
$$


上图中，左图的相关系数为0.9930858，右图的相关系数为0.9573288。左图的相关性比右图的相关性强（点落在直线上的紧密程度）。为什么使用相关系数呢？因为有了强的相关系数，相关系数越大就越能用一条直线将x与y的关系描述出来。通常在做回归分析之前，先来研究变量之间的相关性。若相关系数是在0左右徘徊这进行线性回归分析是没有什么意义的。  因为最后求得结果会是非常不准确的。

### 一元线性回归模型

若X与Y之间存在着较强的相关关系，则有Y=a+bX，若a与b已知，则给出相应的X值，就可以根据方程得到相应Y的预测值。

**1. 建模** 

Y = a + bX + e，其中a为截距项，b为斜率，e为误差项，误差项e的三大假设（服从正太分布、均值为0，方差为常数、相互独立）

**2. 确定参数**

建立方程的目标是：让散点图上的点，`尽可能`的落在建模的直线`附近`。那么用什么衡量`尽可能` 、`附近` 。使用误差平方和衡量预测值与真实值的差距。

![回归分析2](/img/md_imgs/回归分析2.png)
$$
假设真实值为y，预测值为\hat y，那么平方误差为(y-\hat y)^2，寻找合适的参数，使得平方误差和RSS=\sum_{i=1}^{n}(y-\hat y)^2最小。\\  
使用最小二乘法RSS=\sum_{i=1}^{n}(y_i-\hat y_i)^2=\sum_{i=1}^{n}[y_i-(\alpha + \beta x_i)]^2  \\
RSS是关于\alpha、\beta的函数，求RSS的最小值只需要分别对\alpha、\beta求偏导等于0，就可以求得出\alpha、\beta的值，结果为： \\
\beta = \frac{\sum_{i=1}^{n}(X_i - \overline X)(Y_i - \overline Y)}{\sum_{i=1}^{n}(X_i - \overline X)^2} \\
\alpha = \overline Y - \beta \overline X \\
因为直线一定经过（\overline X，\overline Y）点，带入上述\beta等式中即可得到\alpha。
$$

由于总体未知，采用样本值估算（大写的表示总体，而样本采用小写。）：

![回归分析3](/img/md_imgs/回归分析3.png)
$$
b = \hat \beta = \frac{\sum_{i=1}^{n}(x_i - \overline x)(y_i - \overline y)}{\sum_{i=1}^{n}(x_i - \overline x)^2} \\
a = \hat \alpha = \overline y - \beta \overline x \\
$$
从而对于每一个xi,我们可以通过$\hat y = a + bx_i$预测相应的y值。



**范例** 

x=（1， 2， 3， 4），y=（6，5，7,10），构建y关于x的回归方程$y=\alpha + \beta x$ ，使用最小二乘法求解参数：

![回归分析4](/img/md_imgs/回归分析4.png)
$$
b = \hat \beta = \frac{\sum_{i=1}^{n}(x_i - \overline x)(y_i - \overline y)}{\sum_{i=1}^{n}(x_i - \overline x)^2} = 1.4 \\
a = \hat \alpha = \overline y - \beta \overline x = 3.5 \\
$$
得到方程y=3.5 + 1.4x，如果有新的点，则我们会预测相应的y值为：3.5 + 1.4\*2.5 = 7

### 多元线性回归模型

当Y值得影响因素不唯一时，采用多元线性回归模型，得到$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_mX_m + \varepsilon$ ，多元线性回归于一样线性回归的思想是一样的，同样需要通过偏导数求RSS的极小值。通过最小二乘法，计算$RSS=\sum_{i=1}^{n}(y_i - \hat y_i)^2$ 的最小值，该函数式关于$\beta_i$的函数，分别对$\beta_i$ 求偏导并令偏导等于0，可以解出相应的$\beta_i$ 的值。

![回归分析5](/img/md_imgs/回归分析5.png)
$$
\hat \beta = (X^TX)^-1X^Ty
$$

### 范例

```python
# -*- coding: utf-8 -*-
from numpy import *
import pandas as pd
import seaborn as sns
import matplotlib
###线性回归####
#读取数据
data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)
data.tail()
#        TV  radio  newspaper  sales
#196   38.2    3.7       13.8    7.6
#197   94.2    4.9        8.1    9.7
#198  177.0    9.3        6.4   12.8
#199  283.6   42.0       66.2   25.5
#200  232.1    8.6        8.7   13.4

#画散点图
sns.pairplot(data, x_vars=['TV','radio','newspaper'], y_vars='sales', size=7, aspect=0.8)
sns.pairplot(data, x_vars=['TV','radio','newspaper'], y_vars='sales', size=7, aspect=0.8, kind='reg')

#计算相关系数矩阵
data.corr()
#                 TV     radio  newspaper     sales
#TV         1.000000  0.054809   0.056648  0.782224
#radio      0.054809  1.000000   0.354104  0.576223
#newspaper  0.056648  0.354104   1.000000  0.228299
#sales      0.782224  0.576223   0.228299  1.000000

#构建X、Y数据集
X = data[['TV', 'radio', 'newspaper']]
y = data['sales']

##直接根据系数矩阵公式计算
#B = (X.T*X)^(-1)(X.T*Y)
def standRegres(xArr,yArr):
    xMat = mat(xArr); yMat = mat(yArr).T
    xTx = xMat.T*xMat
    #判断行列式为零，举证是奇异矩阵，不能求逆
    if linalg.det(xTx) == 0.0:
        print("This matrix is singular, cannot do inverse")
        return
    ws = xTx.I * (xMat.T*yMat)
    return ws


#求解回归方程系数
X2=X
#增加截距项，默认设置a=1
X2['intercept']=[1]*200
#      TV  radio  newspaper  intercept
#1  230.1   37.8       69.2          1
#2   44.5   39.3       45.1          1
#3   17.2   45.9       69.3          1
#4  151.5   41.3       58.5          1
#5  180.8   10.8       58.4          1
standRegres(X2,y)
#matrix([[  4.57646455e-02],
#        [  1.88530017e-01],
#        [ -1.03749304e-03],
#        [  2.93888937e+00]])


##利用现有库求解
from sklearn.linear_model import LinearRegression
linreg = LinearRegression()
#对自变量和因变量进行拟合
linreg.fit(X, y)

#截距的估计
print(linreg.intercept_)
#系数的估计
print(linreg.coef_)
print(Series({x:y for (x,y) in zip(['TV','Radio','Newspaper'], linreg.coef_)}))
#Newspaper   -0.001037
#Radio        0.188530
#TV           0.045765

##测试集和训练集的构建
#
#如果过度依赖源数据会产生过度拟合的现象，即对以前的训练数据预测准确度高
#但是对新数据的预测的准确性却非常低。为此需要划分测试集和训练集。
# from sklearn.cross_validation import train_test_split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
linreg.fit(X_train, y_train)

#截距的估计
print(linreg.intercept_)
#系数的估计
print(linreg.coef_)
print(Series({x:y for (x,y) in zip(['TV','Radio','Newspaper'], linreg.coef_)}))

#预测
y_pred = linreg.predict(X_test)
#误差评估
from sklearn import metrics

# 计算平均绝对误差
print("MAE:",metrics.mean_absolute_error(y_test,y_pred))
#计算误差平方和
print("MSE:",metrics.mean_squared_error(y_test,y_pred))
# 对误差平方和开方
print("RMSE:",np.sqrt(metrics.mean_squared_error(y_test,y_pred)))
#MAE: 1.06689170826
#MSE: 1.97304562023
#RMSE: 1.40465142303

##模型2，因为newspaper相关性不打，剔除newspaper变量，重新建立模型
feature_cols = ['TV', 'radio']
X = data[feature_cols]
y = data.sales
#划分训练集，测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
#训练
linreg.fit(X_train, y_train)
#预测
y_pred = linreg.predict(X_test)

# 计算平均绝对误差
print("MAE:",metrics.mean_absolute_error(y_test,y_pred))
#计算误差平方和
print("MSE:",metrics.mean_squared_error(y_test,y_pred))
# 对误差平方和开方
print("RMSE:",np.sqrt(metrics.mean_squared_error(y_test,y_pred)))
#MAE: 1.04775904112
#MSE: 1.92627604187
#RMSE: 1.38790346994
```

[^1]: [你应该要掌握的7种回归分析方法](http://blog.csdn.net/lynnucas/article/details/47948639)