---
layout:     post
title:      "Pandas数据聚合与分组分析"
subtitle:   
date:       2018-01-07
author:     "Govind"
header-img: "img/post_bg/11.jpg"
tags:
    - Pandas
---




# Pandas数据聚合与分组分析

## 一、分组分析

是指根据分组字段，将分 析对象划分成不同的部分，以进行对比分析各组之间的差异性的一种方法。常用的统计指标：计数size、求和sum、平均值mean。

**groupby(*by=None*, *axis=0*, *level=None*, *as_index=True*, *sort=True*, *group_keys=True*, *squeeze=False*, \*\*\*kwargs*)**

- by : 若by为函数则会对每一行的数据调用一次，若为by为dict或Series则会用于确定分组，若by为ndarray则会根据as-is区分组，by为str或list时则会按列分组。
- as_index： return object with group labels as the index



根据一个或多个键拆分pandas对象，可以使用plit-apply-combine（拆分，应用，合并）对数据进行拆分聚合统计。

![split-apply-combine](E:/gaocn.github.io/img/md_imgs/split-apply-combine.png)

分组键

- 列表或数据，其长度与待分组的轴一样
- 表示DataFrame某个列名的值
- 字典或Series，给出待分组轴上的值与分组名之间的对应关系
- 函数，用于处理轴索引或索引中的各个标签

```python
# -*- coding: utf-8 -*-
from __future__ import division
from numpy.random import randn
import numpy as np
import os
import matplotlib.pyplot as plt
plt.rc('figure', figsize=(10, 6))
from pandas import Series, DataFrame
import pandas as pd
np.set_printoptions(precision=4)

### GroupBy 技术
df = DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],
                'key2' : ['one', 'two', 'one', 'two', 'one'],
                'data1' : np.random.randn(5),
                'data2' : np.random.randn(5)})

grouped = df['data1'].groupby(df['key1'])
#<pandas.core.groupby.SeriesGroupBy object at 0x0000014F1D7FF1D0>
grouped.mean()
#key1  求均值时，对于非数值数据不进行计算
#a   -0.501573
#b    0.027613

means = df['data1'].groupby([df['key1'], df['key2']]).mean()
"""
key1  key2
a     one    -0.174948
      two    -1.154824
b     one    -1.274116
      two     1.329342
"""
means.unstack()
"""
key2       one       two
key1
a    -0.174948 -1.154824
b    -1.274116  1.329342
"""

states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])
years = np.array([2005, 2005, 2006, 2005, 2006])
df['data1'].groupby([states, years]).mean()
"""
California  2005   -1.154824
            2006   -1.274116
Ohio        2005    0.147064
            2006    0.685317
"""
df.groupby('key1').mean()
df.groupby(['key1', 'key2']).mean()
"""
              data1     data2
key1 key2
a    one  -0.174948  0.394913
     two  -1.154824  1.879570
b    one  -1.274116  1.156108
     two   1.329342 -0.433824
"""
df.groupby(['key1', 'key2']).size()
```

### 1.1 对分组进行迭代

```python
# ### 对分组进行迭代
for name, group in df.groupby('key1'):
    print(name)
    print(group)
"""
a
      data1     data2 key1 key2
0 -1.035213  0.973977    a  one
1 -1.154824  1.879570    a  two
4  0.685317 -0.184152    a  one
b
      data1     data2 key1 key2
2 -1.274116  1.156108    b  one
3  1.329342 -0.433824    b  two
"""

for (k1, k2), group in df.groupby(['key1', 'key2']):
    print((k1, k2))
    print(group)
"""
('a', 'one')
      data1     data2 key1 key2
0 -1.035213  0.973977    a  one
4  0.685317 -0.184152    a  one
('a', 'two')
      data1    data2 key1 key2
1 -1.154824  1.87957    a  two
('b', 'one')
      data1     data2 key1 key2
2 -1.274116  1.156108    b  one
('b', 'two')
      data1     data2 key1 key2
3  1.329342 -0.433824    b  two
"""
pieces = dict(list(df.groupby('key1')))
pieces['b']
"""
{'a':       data1     data2 key1 key2
 0 -1.035213  0.973977    a  one
 1 -1.154824  1.879570    a  two
 4  0.685317 -0.184152    a  one, 'b':       data1     data2 key1 key2
 2 -1.274116  1.156108    b  one
 3  1.329342 -0.433824    b  two}
"""
#groupby默认是按照行axis=0分组
grouped = df.groupby(df.dtypes, axis=1)
dict(list(grouped))
```

### 1.2 选取分组后一个或一组列

```python
# ### 选择一个或一组列
df.groupby('key1')['data1']
df.groupby('key1')[['data2']]
#只对部分列进行聚合，节省内存
df['data1'].groupby(df['key1'])
df[['data2']].groupby(df['key1'])

df.groupby(['key1', 'key2'])[['data2']].mean()
"""
              data2
key1 key2
a    one   0.394913
     two   1.879570
b    one   1.156108
     two  -0.433824
"""
s_grouped = df.groupby(['key1', 'key2'])['data2']
s_grouped.mean()
"""
key1  key2
a     one     0.394913
      two     1.879570
b     one     1.156108
      two    -0.433824
"""
```

### 1.3 通过字典或Series分组

```python
# ### 通过字典或series进行分组
people = DataFrame(np.random.randn(5, 5),
                   columns=['a', 'b', 'c', 'd', 'e'],
                   index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])
people.ix[2:3, ['b', 'c']] = np.nan # Add a few NA values
"""
               a         b         c         d         e
Joe     0.904551 -1.167257  1.620679  0.468020  0.415050
Steve   0.221909  0.826869 -1.427539  0.323660  0.618976
Wes     1.448292       NaN       NaN  0.496856 -0.271655
Jim    -1.101991 -1.399487  0.272109 -0.873881  0.087034
Travis -1.034796  0.383681 -0.851143 -0.642471 -0.161579
"""
mapping = {'a': 'red', 'b': 'red', 'c': 'blue',
           'd': 'blue', 'e': 'red', 'f' : 'orange'}

by_column = people.groupby(mapping, axis=1)
by_column.sum()
"""
            blue       red
Joe     2.088699  0.152343
Steve  -1.103880  1.667753
Wes     0.496856  1.176637
Jim    -0.601772 -2.414444
Travis -1.493614 -0.812694
"""
map_series = Series(mapping)
people.groupby(map_series, axis=1).count()
"""
        blue  red
Joe        2    3
Steve      2    3
Wes        1    2
Jim        2    3
Travis     2    3
"""

#        data1   data2    key1   key2
#0   0.233689    0.139407    a   one
#1   0.491570    0.366669    a   two
grouped2 = df.groupby(['a', 'a', 'b', 'b', 'a'])
#无论是['a', 'a', 'b', 'b', 'a']，
#还是['1', '1', '2', '2', '1']，
#都是把第1、2、和5行分为一组，把第3行和第4行分为一组
grouped2 = df.groupby(['1', '1', '2', '2', '1'])
```

### 1.4 通过函数分组

```python
# ### 通过函数进行分组
people.groupby(len).sum()

key_list = ['one', 'one', 'one', 'two', 'two']
people.groupby([len, key_list]).min()
"""
              a         b         c         d         e
3 one  0.904551 -1.167257  1.620679  0.468020 -0.271655
  two -1.101991 -1.399487  0.272109 -0.873881  0.087034
5 one  0.221909  0.826869 -1.427539  0.323660  0.618976
6 two -1.034796  0.383681 -0.851143 -0.642471 -0.161579
"""
```

### 1.5 根据索引级别分组

```python
# ### 通过索引进行分组
columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],
                                    [1, 3, 5, 1, 3]], names=['cty', 'tenor'])
hier_df = DataFrame(np.random.randn(4, 5), columns=columns)
"""
cty          US                            JP
tenor         1         3         5         1         3
0     -1.822717 -0.494550  0.673479  0.136220  1.419704
1     -1.546854  1.216669  1.020965 -0.050591  0.142995
2     -0.917939  1.601968 -0.729515 -0.365755 -1.097587
3      1.628180 -1.146297 -1.695819 -0.621583 -0.749737
"""

hier_df.groupby(level='cty', axis=1).count()
"""
cty  JP  US
0     2   3
1     2   3
2     2   3
3     2   3
"""
```

## 二、聚合（分组统计）

聚合：任何能够从数组产生标量值的数据转换过程

**分组统计函数：groupby(by=[分组列1, 分组列2, ...])[统计列1, 统计列2, ... ].agg({统计列名1: 统计函数1,  统计列名2: 统计函数2, ...})**

- by：用于分组的列
- 中括号：用于统计的列
- agg：统计别名显示统计值的名称，统计函数用于统计数据


| 函数名        | 说明               |
| ---------- | ---------------- |
| count      | 分组中非NA值得数量       |
| sum        | 非NA值得和           |
| mean       | 非NA值得平均值         |
| median     | 非NA值的算术中位数       |
| std、var    | 无偏（分为n-1）的标准差和方差 |
| min、max    | 非NA值的最小值和最大值     |
| prod       | 非NA值得积           |
| first、last | 第一个、最后一个非NA值     |

```python
# ##数据聚合
grouped = df.groupby('key1')
grouped['data1'].quantile(0.9)
"""
key1
a    0.341211
b    1.068996
"""
#按照自定义函数聚合
def peak_to_peak(arr):
    return arr.max() - arr.min()
grouped.agg(peak_to_peak)
"""
         data1     data2
key1
a     1.840141  2.063721
b     2.603457  1.589932
"""
grouped.describe()
"""
               data1     data2
key1
a    count  3.000000  3.000000
     mean  -0.501573  0.889798
     std    1.029616  1.034433
     min   -1.154824 -0.184152
     25%   -1.095019  0.394913
     50%   -1.035213  0.973977
     75%   -0.174948  1.426773
     max    0.685317  1.879570
b    count  2.000000  2.000000
     mean   0.027613  0.361142
     std    1.840922  1.124252
     min   -1.274116 -0.433824
     25%   -0.623251 -0.036341
     50%    0.027613  0.361142
     75%    0.678477  0.758625
     max    1.329342  1.156108
"""

# ### 面向列的多函数应用
tips = pd.read_csv('d:/data/tips.csv')
tips['tip_pct'] = tips['tip'] / tips['total_bill']
tips.head()
"""
   total_bill   tip     sex smoker  day    time  size   tip_pct
0       16.99  1.01  Female     No  Sun  Dinner     2  0.059447
1       10.34  1.66    Male     No  Sun  Dinner     3  0.160542
2       21.01  3.50    Male     No  Sun  Dinner     3  0.166587
3       23.68  3.31    Male     No  Sun  Dinner     2  0.139780
4       24.59  3.61  Female     No  Sun  Dinner     4  0.146808
5       25.29  4.71    Male     No  Sun  Dinner     4  0.186240
"""
grouped = tips.groupby(['sex', 'smoker'])
grouped_pct = grouped['tip_pct']
grouped_pct.agg('mean') #比直接使用groupby().mean()速度要慢很多
"""
sex     smoker
Female  No        0.156921
        Yes       0.182150
Male    No        0.160669
        Yes       0.152771
"""

grouped_pct.agg(['mean', 'std', peak_to_peak])
"""
                   mean       std  peak_to_peak
sex    smoker
Female No      0.156921  0.036421      0.195876
       Yes     0.182150  0.071595      0.360233
Male   No      0.160669  0.041849      0.220186
       Yes     0.152771  0.090588      0.674707
"""
grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])
"""
                    foo       bar
sex    smoker
Female No      0.156921  0.036421
       Yes     0.182150  0.071595
Male   No      0.160669  0.041849
       Yes     0.152771  0.090588
"""
functions = ['count', 'mean', 'max']
result = grouped['tip_pct', 'total_bill'].agg(functions)
"""
              tip_pct                     total_bill
                count      mean       max      count       mean    max
sex    smoker
Female No          54  0.156921  0.252672         54  18.105185  35.83
       Yes         33  0.182150  0.416667         33  17.977879  44.30
Male   No          97  0.160669  0.291990         97  19.791237  48.33
       Yes         60  0.152771  0.710345         60  22.284500  50.81
"""
result['tip_pct']

ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]
grouped['tip_pct', 'total_bill'].agg(ftuples)
"""
                   tip_pct              total_bill
              Durchschnitt Abweichung Durchschnitt Abweichung
sex    smoker
Female No         0.156921   0.001327    18.105185  53.092422
       Yes        0.182150   0.005126    17.977879  84.451517
Male   No         0.160669   0.001751    19.791237  76.152961
       Yes        0.152771   0.008206    22.284500  98.244673
"""

grouped.agg({'tip' : np.max, 'size' : 'sum'})
"""
                tip  size
sex    smoker
Female No       5.2   140
       Yes      6.5    74
Male   No       9.0   263
       Yes     10.0   150
"""
grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'],
             'size' : 'sum'})
"""
                tip_pct                               size
                    min       max      mean       std  sum
sex    smoker
Female No      0.056797  0.252672  0.156921  0.036421  140
       Yes     0.056433  0.416667  0.182150  0.071595   74
Male   No      0.071804  0.291990  0.160669  0.041849  263
       Yes     0.035638  0.710345  0.152771  0.090588  150
"""
```

### 2.1 范例

```python
import numpy;
from pandas import read_csv;
data = read_csv('D:\\PA\\8.2\\data.csv');
data['score2'] = data['score']*2
data.groupby(by=['class'])['score'].agg({
    '总分':numpy.sum, 
    '人数':numpy.size, 
    '平均值':numpy.mean, 
    '方差':numpy.var, 
    '标准差':numpy.std
})
data.groupby(by=['class', 'name'])['score''].agg([
    numpy.size, 
    numpy.sum
])
#         size  average
#class
#english     1       93
#math        1       78
#wuwen       1       56

result = data.groupby(by=['class'])['score'].agg({
    '总分':numpy.sum, 
    '人数':numpy.size, 
    '平均值':numpy.mean, 
    '方差':numpy.var, 
    '标准差':numpy.std
})
result.index
result.columns
result['平均值']
result2 = data.groupby(by=['class'])[['point', 'score']].agg([
    numpy.size, 
    numpy.sum
])
#        point     score
#         size sum  size   sum
#class
#english     1  93   1.0  78.0
#math        1  78   1.0  54.6
#wuwen       1  56   1.0  23.4

result2.columns
#MultiIndex(levels=[['point', 'score'], ['size', 'sum']],
#           labels=[[0, 0, 1, 1], [0, 1, 0, 1]])
result2['score']
result2['score']['sum']
result.reset_index()   #将原来的index作为单独的列
#     class point     score
#            size sum  size   sum
#0  english     1  93   1.0  78.0
#1     math     1  78   1.0  54.6
#2    wuwen     1  56   1.0  23.4
```

### 2.2 对分组进行transform和apply

#### 2.2.1 transform对分组结果进行转换

transform函数允许对分组后的数据进行自定义的转换，agg是transform的一种应用。

```python
###分组级运算和转换(利用apply、transform函数自定义转换，agg只是转换应用中的一种)
k1_means = df.groupby('key1').mean().add_prefix('mean_')
"""
      mean_data1  mean_data2
key1
a      -0.501573    0.889798
b       0.027613    0.361142
"""
pd.merge(df, k1_means, left_on='key1', right_index=True)
"""
      data1     data2 key1 key2  mean_data1  mean_data2
0 -1.035213  0.973977    a  one   -0.501573    0.889798
1 -1.154824  1.879570    a  two   -0.501573    0.889798
4  0.685317 -0.184152    a  one   -0.501573    0.889798
2 -1.274116  1.156108    b  one    0.027613    0.361142
3  1.329342 -0.433824    b  two    0.027613    0.361142
"""
people
"""
               a         b         c         d         e
Joe     0.904551 -1.167257  1.620679  0.468020  0.415050
Steve   0.221909  0.826869 -1.427539  0.323660  0.618976
Wes     1.448292       NaN       NaN  0.496856 -0.271655
Jim    -1.101991 -1.399487  0.272109 -0.873881  0.087034
Travis -1.034796  0.383681 -0.851143 -0.642471 -0.161579
"""

key = ['one', 'two', 'one', 'two', 'one']
people.groupby(key).mean()
"""
            a         b         c         d         e
one  0.439349 -0.391788  0.384768  0.107468 -0.006061
two -0.440041 -0.286309 -0.577715 -0.275111  0.353005
"""
#
#将函数应用到各个分组里面，并用结果填充到对应位置
#
people.groupby(key).transform(np.mean)
"""
               a         b         c         d         e
Joe     0.439349 -0.391788  0.384768  0.107468 -0.006061
Steve  -0.440041 -0.286309 -0.577715 -0.275111  0.353005
Wes     0.439349 -0.391788  0.384768  0.107468 -0.006061
Jim    -0.440041 -0.286309 -0.577715 -0.275111  0.353005
Travis  0.439349 -0.391788  0.384768  0.107468 -0.006061
"""

def demean(arr):
    return arr - arr.mean()
demeaned = people.groupby(key).transform(demean)
"""
               a         b         c         d         e
Joe     0.465202 -0.775469  1.235911  0.360552  0.421111
Steve   0.661950  1.113178 -0.849824  0.598770  0.265971
Wes     1.008943       NaN       NaN  0.389387 -0.265594
Jim    -0.661950 -1.113178  0.849824 -0.598770 -0.265971
Travis -1.474145  0.775469 -1.235911 -0.749939 -0.155518
"""
demeaned.groupby(key).mean()
"""
       a    b    c    d             e
one  0.0  0.0  0.0  0.0 -9.251859e-18
two  0.0  0.0  0.0  0.0  0.000000e+00
"""
```

#### 2.2.2 apply处理分组后的值

apply中一般性的“拆分-应用-合并”，类属于'map-reduce'中的reduce阶段，对数据数据进行处理。

```python
# ### apply方法
def top(df, n=5, column='tip_pct'):
    """
    FutureWarning: by argument to sort_index 
          is deprecated, pls use .sort_values(by=...)
    """
    return df.sort_index(by=column)[-n:]
"""
     total_bill   tip     sex smoker  day    time  size   tip_pct
109       14.31  4.00  Female    Yes  Sat  Dinner     2  0.279525
183       23.17  6.50    Male    Yes  Sun  Dinner     4  0.280535
232       11.61  3.39    Male     No  Sat  Dinner     2  0.291990
67         3.07  1.00  Female    Yes  Sat  Dinner     1  0.325733
178        9.60  4.00  Female    Yes  Sun  Dinner     2  0.416667
172        7.25  5.15    Male    Yes  Sun  Dinner     2  0.710345
"""
tips.groupby('smoker').apply(top)
"""
            total_bill   tip     sex smoker   day    time  size   tip_pct
smoker
No     88        24.71  5.85    Male     No  Thur   Lunch     2  0.236746
       185       20.69  5.00    Male     No   Sun  Dinner     5  0.241663
       51        10.29  2.60  Female     No   Sun  Dinner     2  0.252672
       149        7.51  2.00    Male     No  Thur   Lunch     2  0.266312
       232       11.61  3.39    Male     No   Sat  Dinner     2  0.291990
Yes    109       14.31  4.00  Female    Yes   Sat  Dinner     2  0.279525
       183       23.17  6.50    Male    Yes   Sun  Dinner     4  0.280535
       67         3.07  1.00  Female    Yes   Sat  Dinner     1  0.325733
       178        9.60  4.00  Female    Yes   Sun  Dinner     2  0.416667
       172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345
"""
tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')
result = tips.groupby('smoker')['tip_pct'].describe()
result.unstack('smoker')

#f = lambda x: x.describe()
#grouped.apply(f)

#  禁止分组键
tips.groupby('smoker', group_keys=False).apply(top)
"""
     total_bill   tip     sex smoker   day    time  size   tip_pct
88        24.71  5.85    Male     No  Thur   Lunch     2  0.236746
185       20.69  5.00    Male     No   Sun  Dinner     5  0.241663
51        10.29  2.60  Female     No   Sun  Dinner     2  0.252672
149        7.51  2.00    Male     No  Thur   Lunch     2  0.266312
232       11.61  3.39    Male     No   Sat  Dinner     2  0.291990
109       14.31  4.00  Female    Yes   Sat  Dinner     2  0.279525
183       23.17  6.50    Male    Yes   Sun  Dinner     4  0.280535
67         3.07  1.00  Female    Yes   Sat  Dinner     1  0.325733
178        9.60  4.00  Female    Yes   Sun  Dinner     2  0.416667
172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345
"""

# ### 用特定于分组的值填充缺失值
s = Series(np.random.randn(6))
s[::2] = np.nan
"""
0         NaN
1    0.097088
2         NaN
3    0.685712
4         NaN
5    0.222743
"""
#用列的均值对缺失值进行填充
s.fillna(s.mean())

states = ['Ohio', 'New York', 'Vermont', 'Florida',
          'Oregon', 'Nevada', 'California', 'Idaho']
group_key = ['East'] * 4 + ['West'] * 4
data = Series(np.random.randn(8), index=states)
data[['Vermont', 'Nevada', 'Idaho']] = np.nan
"""
Ohio          0.595227
New York     -0.099267
Vermont            NaN
Florida       0.291544
Oregon        0.694426
Nevada             NaN
California    0.768202
Idaho              NaN
"""

data.groupby(group_key).mean()
"""
East    0.262502
West    0.731314
"""

#对不同列的缺失值填充不同的值
fill_mean = lambda g: g.fillna(g.mean())
data.groupby(group_key).apply(fill_mean)
"""
Ohio          0.595227
New York     -0.099267
Vermont       0.262502
Florida       0.291544
Oregon        0.694426
Nevada        0.731314
California    0.768202
Idaho         0.731314
"""
fill_values = {'East': 0.5, 'West': -1}
fill_func = lambda g: g.fillna(fill_values[g.name])
data.groupby(group_key).apply(fill_func)


# ### 随机采样和排列
suits = ['H', 'S', 'C', 'D']
card_val = (range(1, 11) + [10] * 3) * 4
base_names = ['A'] + range(2, 11) + ['J', 'K', 'Q']
cards = []
for suit in ['H', 'S', 'C', 'D']:
    cards.extend(str(num) + suit for num in base_names)
deck = Series(card_val, index=cards)

def draw(deck, n=5):
    return deck.take(np.random.permutation(len(deck))[:n])

get_suit = lambda card: card[-1] #只要最后一个字母
deck.groupby(get_suit).apply(draw, n=2)
"""
C  9C      9
   QC     10
D  KD     10
   AD      1
H  AH      1
   10H    10
S  5S      5
   4S      4
"""
#不显示分组关键字
deck.groupby(get_suit, group_keys=False).apply(draw, n=2)
# ### 分组加权平均数和相关系数
df = DataFrame({'category': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'],
                'data': np.random.randn(8),
                'weights': np.random.rand(8)})
grouped = df.groupby('category')
get_wavg = lambda g: np.average(g['data'], weights=g['weights'])
grouped.apply(get_wavg)
"""
category
a    0.203903
b    0.187173
"""
close_px = pd.read_csv('d:/data/stock_px.csv', parse_dates=True, index_col=0)
close_px.info()
"""
              AAPL   MSFT    XOM      SPX
2011-10-11  400.29  27.00  76.27  1195.54
2011-10-12  402.19  26.96  77.16  1207.25
2011-10-13  408.43  27.18  76.37  1203.66
2011-10-14  422.00  27.27  78.11  1224.58
"""
rets = close_px.pct_change().dropna()
spx_corr = lambda x: x.corrwith(x['SPX'])
by_year = rets.groupby(lambda x: x.year)
by_year.apply(spx_corr)
"""
          AAPL      MSFT       XOM  SPX
2003  0.541124  0.745174  0.661265  1.0
2004  0.374283  0.588531  0.557742  1.0
2005  0.467540  0.562374  0.631010  1.0
2006  0.428267  0.406126  0.518514  1.0
2007  0.508118  0.658770  0.786264  1.0
2008  0.681434  0.804626  0.828303  1.0
2009  0.707103  0.654902  0.797921  1.0
2010  0.710105  0.730118  0.839057  1.0
2011  0.691931  0.800996  0.859975  1.0
"""
# 苹果公司和微软的年度相关系数
by_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))
"""
2003    0.480868
2004    0.259024
2005    0.300093
2006    0.161735
2007    0.417738
"""
```

## 三、分布分析

根据分析目的，将数据（定量数据）进行等距或者不等距的分组，进行研究各组分布规律的一种分析方法。grouo

```python
# ### 分位数和桶分析
frame = DataFrame({'data1': np.random.randn(1000),
                   'data2': np.random.randn(1000)})
factor = pd.cut(frame.data1, 4)
"""
0     (0.117, 1.756]
1    (-1.523, 0.117]
2     (0.117, 1.756]
3    (-1.523, 0.117]
4    (-1.523, 0.117]
Name: data1, dtype: category
Categories (4, object): [(-3.168, -1.523] < (-1.523, 0.117] < (0.117, 1.756] < (1.756, 3.395]]
"""

def get_stats(group):
    return {'min': group.min(), 'max': group.max(),
            'count': group.count(), 'mean': group.mean()}

grouped = frame.data2.groupby(factor)
grouped.apply(get_stats).unstack()
"""
                  count       max      mean       min
data1
(-3.168, -1.523]   75.0  2.931585  0.065967 -2.437032
(-1.523, 0.117]   489.0  2.896323  0.008053 -3.537383
(0.117, 1.756]    395.0  3.014159  0.039974 -2.732654
(1.756, 3.395]     41.0  2.112763 -0.005096 -2.482383
"""
grouping = pd.qcut(frame.data1, 10, labels=False)
grouped = frame.data2.groupby(grouping)
grouped.apply(get_stats).unstack()
"""
       count       max      mean       min
data1
0      100.0  2.931585  0.027635 -2.437032
1      100.0  2.104165 -0.006459 -2.146599
2      100.0  2.896323  0.043083 -3.537383
3      100.0  2.200706  0.067105 -2.964531
4      100.0  2.428681  0.011515 -2.822976
5      100.0  2.328190 -0.080532 -2.562256
6      100.0  3.014159  0.062072 -1.858570
7      100.0  1.875662 -0.026615 -2.452683
8      100.0  2.687377  0.052647 -2.732654
9      100.0  2.575786  0.094209 -2.482383
"""

data = read_csv('D:\\PA\\8.3\\data.csv');
bins = [min(data.年龄)-1, 20, 30, 40, max(data.年龄)+1];
labels = ['20岁以及以下', '21岁到30岁', '31岁到40岁', '41岁以上'];
年龄分层 = pandas.cut(data.年龄, bins, labels=labels)
data['年龄分层'] = 年龄分层;
data.groupby(by=['年龄分层'])['年龄'].agg({'人数':numpy.size})
```

## 四、交叉分析(透视表、交叉表)

通常用于分析两个或两个以上，分组变量之间的关系，以交叉表形式进行变量间关系的对比分析，可以分为：`1.定量、定量分组交叉` ; `2.定量、定性分组交叉` ；`3. 定性、定性分组交叉` 。

**交叉计数函数：DataFrame.pivot_table(values, index, columns, aggfunc='mean', fill_value=None,margins=False, dropna=True)**

- values：待聚合的列名称，默认聚合所有数值列
- index：用于分组的列名或其他分组键，出现在结果透视表的行
- columns：用于分组的列名或其他分组键，出现在结果透视表的列
- aggfunc： 聚合函数或函数列表，默认为'mean'，可以是任何对groupby有效的函数，若列值不是数值型，这不进行聚合操作
- fill_value：用于替换结果表中的缺失值
- margins：添加行/列小计和总计，默认为False

**pandas.crosstab(index, columns, values=None, aggfunc=None, margins=False, margins_name='All', dropna=True, normalize=False)**

- index：用于分组的列名或其他分组键，出现在结果行
- columns：用于分组的列名或其他分组键，出现在结果行
- values：待聚合的列名称，默认聚合所有数值列

```python
### 透视表
tips.pivot_table(index=['sex', 'smoker'])
"""
                   size       tip   tip_pct  total_bill
sex    smoker
Female No      2.592593  2.773519  0.156921   18.105185
       Yes     2.242424  2.931515  0.182150   17.977879
Male   No      2.711340  3.113402  0.160669   19.791237
       Yes     2.500000  3.051167  0.152771   22.284500
"""
tips.pivot_table(['tip_pct', 'size'], index=['sex', 'day'],
                 columns='smoker')
"""
              tip_pct                size
smoker             No       Yes        No       Yes
sex    day
Female Fri   0.165296  0.209129  2.500000  2.000000
       Sat   0.147993  0.163817  2.307692  2.200000
       Sun   0.165710  0.237075  3.071429  2.500000
       Thur  0.155971  0.163073  2.480000  2.428571
Male   Fri   0.138005  0.144730  2.000000  2.125000
       Sat   0.162132  0.139067  2.656250  2.629630
       Sun   0.158291  0.173964  2.883721  2.600000
       Thur  0.165706  0.164417  2.500000  2.300000
"""
tips.pivot_table(['tip_pct', 'size'], index=['sex', 'day'],
                 columns='smoker', margins=True)
"""
              tip_pct                          size
smoker             No       Yes       All        No       Yes       All
sex    day
Female Fri   0.165296  0.209129  0.199388  2.500000  2.000000  2.111111
       Sat   0.147993  0.163817  0.156470  2.307692  2.200000  2.250000
       Sun   0.165710  0.237075  0.181569  3.071429  2.500000  2.944444
       Thur  0.155971  0.163073  0.157525  2.480000  2.428571  2.468750
Male   Fri   0.138005  0.144730  0.143385  2.000000  2.125000  2.100000
       Sat   0.162132  0.139067  0.151577  2.656250  2.629630  2.644068
       Sun   0.158291  0.173964  0.162344  2.883721  2.600000  2.810345
       Thur  0.165706  0.164417  0.165276  2.500000  2.300000  2.433333
All          0.159328  0.163196  0.160803  2.668874  2.408602  2.569672
"""
tips.pivot_table('tip_pct', index=['sex', 'smoker'], columns='day',
                 aggfunc=np.size, margins=True)
"""
day             Fri   Sat   Sun  Thur    All
sex    smoker
Female No       2.0  13.0  14.0  25.0   54.0
       Yes      7.0  15.0   4.0   7.0   33.0
Male   No       2.0  32.0  43.0  20.0   97.0
       Yes      8.0  27.0  15.0  10.0   60.0
All            19.0  87.0  76.0  62.0  244.0
"""
tips.pivot_table('size', index=['time', 'sex', 'smoker'],
                 columns='day', aggfunc='sum', fill_value=0)

# ### 交叉表
from io import StringIO
data = """Sample    Gender    Handedness
1    Female    Right-handed
2    Male    Left-handed
3    Female    Right-handed
4    Male    Right-handed
5    Male    Left-handed
6    Male    Right-handed
7    Female    Right-handed
8    Female    Left-handed
9    Male    Right-handed
10    Female    Right-handed"""
data = pd.read_table(StringIO(data), sep='\s+')
pd.crosstab(data.Gender, data.Handedness, margins=True)
"""
Handedness  Left-handed  Right-handed  All
Gender
Female                1             4    5
Male                  2             3    5
All                   3             7   10
"""
pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)
"""
smoker        No  Yes  All
time   day
Dinner Fri     3    9   12
       Sat    45   42   87
       Sun    57   19   76
       Thur    1    0    1
Lunch  Fri     1    6    7
       Thur   44   17   61
All          151   93  244
"""
```

## 五、 结构分析

在分组的基础上，计算各组成部分所占的比重，进而分析总体的内部特征的一种分析方法。

数据框的外运算：add、sub、multiply、div

数据框的内运算：sum、mean、var、std等

```python
import numpy;
from pandas import read_csv;
data = read_csv('D:\\PA\\8.5\\data.csv');
data_pt = data.pivot_table(
    values=['月消费（元）'], 
    index=['省份'], 
    columns=['通信品牌'], 
    aggfunc=[numpy.sum]
);
#axis=0 按列运算， axis=1按行计算
data_pt.sum()
data_pt.sum(axis=0)
data_pt.sum(axis=1)
data_pt.div(data_pt.sum(axis=1), axis=0);
data_pt.div(data_pt.sum(axis=0), axis=1);
```

## 六、相关分析（correlation analysis）

研究现象之间是否存在某种依存关系，并对具有依赖关系的现象探讨其相关方向以及相关程度，是研究随机变量之间的相关关系的一种统计方法。

**相关系数**：可以用来描述定量变量之间的关系。

| 相关系数abs(r)的取值范围     | 相关程度 |
| ------------------- | ---- |
| 0 <= abs(r) < 0.3   | 低度相关 |
| 0.3 <= abs(r) < 0.8 | 中度相关 |
| 0.8 <= abs(r) <= 1  | 高度相关 |

**相关分析函数：DataFrame.corr(method='pearson', min_periods=1),  Series.corr(other，method='pearson', min_periods=None)**

- method：{‘pearson’, ‘kendall’, ‘spearman’}，支持不同的相关系数； min_periods表示进行计算时需要的最少数据量；
- 若由DataFrame调用corr方法，那么将计算每个列两两之间的相似度；由序列Series调用corr方法，那么只是计算该序列与传入的序列之间的相关度；两者都不会考虑缺失值。
- 数据框调用corr返回DataFrame，序列调用corr返回一个数值型，大小为相关度；

**DataFrame.corrwith(other, axis=0, drop=False)**

- 计算不同DataFrame之间的行或列的相关性，drop=false默认返回两者的union结果，若为True会删去missing indices的结果。

```python
# -*- coding: utf-8 -*-
from pandas import read_csv;
data = read_csv('D:\\PA\\8.6\\data.csv');
#先来看看如何进行两个列之间的相关度的计算
data['人口'].corr(data['文盲率'])
#多列之间的相关度的计算方法
#选择多列的方法
#data.loc[:, ['列1', '列2', '……', '列n']]
data.loc[:, ['超市购物率', '网上购物率', '文盲率', '人口']].corr()
```
